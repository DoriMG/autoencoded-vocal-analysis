{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ava.segmenting.utils import copy_segments_to_standard_format\n",
    "from ava.preprocessing.utils import get_spec # makes spectrograms\n",
    "from ava.models.vae import X_SHAPE # spectrogram dimensions\n",
    "from ava.preprocessing.preprocess import tune_syll_preprocessing_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function copy_segments_to_standard_format in module ava.segmenting.utils:\n",
      "\n",
      "copy_segments_to_standard_format(orig_seg_dirs, new_seg_dirs, seg_ext, delimiter, usecols, skiprows, max_duration=None)\n",
      "    Copy onsets/offsets from SAP, MUPET, or Deepsqueak into a standard format.\n",
      "\n",
      "    Note\n",
      "    ----\n",
      "    - `delimiter`, `usecols`, and `skiprows` are all passed to `numpy.loadtxt`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    orig_seg_dirs : list of str\n",
      "            Directories containing original segments.\n",
      "    new_seg_dirs : list of str\n",
      "            Corresponding directories for new segments.\n",
      "    seg_ext : str\n",
      "            Input filename extension.\n",
      "    delimiter : str\n",
      "            Input filename delimiter. For a CSV file, for example, this would be a\n",
      "            comma: `','`\n",
      "    usecols : tuple\n",
      "            Input file onset and offset columns, zero-indexed.\n",
      "    skiprows : int\n",
      "            Number of rows to skip. For example, if there is a single-line header\n",
      "            set `skiprows=1`.\n",
      "    max_duration : {None, float}, optional\n",
      "            Maximum segment duration. If None, no max is set. Defaults to `None`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(copy_segments_to_standard_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_seg_dirs = [\"C:\\\\\\\\Users\\\\door1\\\\Documents\\\\Work\\\\Frankfurt\\\\autoencoded-vocal-analysis\\\\data\\\\c2_br\"]\n",
    "new_seg_dirs =  [\"C:\\\\\\\\Users\\\\door1\\\\Documents\\\\Work\\\\Frankfurt\\\\autoencoded-vocal-analysis\\\\data\\\\c2_br_out\"]\n",
    "seg_ext = '.txt'\n",
    "delimiter  = '\\t'\n",
    "usecols = (0,1)\n",
    "skiprows =0\n",
    "copy_segments_to_standard_format(orig_seg_dirs, new_seg_dirs, seg_ext, delimiter, usecols, skiprows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_params = {\n",
    "    'get_spec': get_spec, # spectrogram maker\n",
    "    'max_dur': 0.4, # maximum syllable duration\n",
    "    'min_freq': 2000, # minimum frequency\n",
    "    'max_freq': 12000, # maximum frequency\n",
    "    'num_freq_bins': X_SHAPE[0], # hard-coded\n",
    "    'num_time_bins': X_SHAPE[1], # hard-coded\n",
    "    'nperseg': 1024, # FFT\n",
    "    'noverlap': 512, # FFT\n",
    "    'spec_min_val': 0.1, # minimum log-spectrogram value\n",
    "    'spec_max_val': 6.0, # maximum log-spectrogram value\n",
    "    'fs': 44100, # audio samplerater\n",
    "    'mel': False, # frequency spacing, mel or linear\n",
    "    'time_stretch': True, # stretch short syllables?\n",
    "    'within_syll_normalize': False, # normalize spectrogram values on a\n",
    "                                    # spectrogram-by-spectrogram basis\n",
    "    'max_num_syllables': None, # maximum number of syllables per directory\n",
    "    'sylls_per_file': 20, # syllable per file\n",
    "    'real_preprocess_params': ('min_freq', 'max_freq', 'spec_min_val', \\\n",
    "            'spec_max_val', 'max_dur'), # tunable parameters\n",
    "    'int_preprocess_params': ('nperseg','noverlap'), # tunable parameters\n",
    "    'binary_preprocess_params': ('time_stretch', 'mel', \\\n",
    "            'within_syll_normalize'), # tunable parameters\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dirs = [\"C:\\\\Users\\\\door1\\\\Documents\\\\Work\\\\Frankfurt\\\\autoencoded-vocal-analysis\\\\data\\\\audio\"] # directories containing audio\n",
    "seg_dirs = new_seg_dirs# directories containing onset/offset decisions\n",
    "#preprocess_params = tune_syll_preprocessing_params(audio_dirs, seg_dirs, preprocess_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_dirs = [\"C:\\\\Users\\\\door1\\\\Documents\\\\Work\\\\Frankfurt\\\\autoencoded-vocal-analysis\\\\data\\\\specs\"]\n",
    "\n",
    "from ava.preprocessing.preprocess import process_sylls\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import repeat\n",
    "\n",
    "gen = zip(audio_dirs, seg_dirs, spec_dirs, repeat(preprocess_params))\n",
    "Parallel(n_jobs=4)(delayed(process_sylls)(*args) for args in gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8 # 80/20 train/test split\n",
    "\n",
    "# Construct a random train/test partition.\n",
    "from ava.models.vae_dataset import get_syllable_partition\n",
    "partition = get_syllable_partition(spec_dirs, split)\n",
    "\n",
    "# Make Dataloaders.\n",
    "from ava.models.vae_dataset import get_syllable_data_loaders\n",
    "loaders = get_syllable_data_loaders(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Training: epochs 0 to 100\n",
      "Training set: 720\n",
      "Test set: 180\n",
      "========================================\n",
      "Epoch: 0 Average loss: 107561.6418\n",
      "Test loss: 5982.7954\n",
      "Epoch: 1 Average loss: 35918.6121\n",
      "Epoch: 2 Average loss: 19636.1168\n",
      "Test loss: 8430.4753\n",
      "Epoch: 3 Average loss: 12922.3218\n",
      "Epoch: 4 Average loss: 9433.6224\n",
      "Test loss: 7491.6844\n",
      "Epoch: 5 Average loss: 7403.5926\n",
      "Epoch: 6 Average loss: 6126.9557\n",
      "Test loss: 5237.5189\n",
      "Epoch: 7 Average loss: 5218.0044\n",
      "Epoch: 8 Average loss: 4429.8805\n",
      "Test loss: 3816.0930\n",
      "Epoch: 9 Average loss: 3712.3244\n",
      "Epoch: 10 Average loss: 3074.0919\n",
      "Test loss: 2745.8310\n",
      "Epoch: 11 Average loss: 2554.6710\n",
      "Epoch: 12 Average loss: 2190.2065\n",
      "Test loss: 1811.3368\n",
      "Epoch: 13 Average loss: 1916.2137\n",
      "Epoch: 14 Average loss: 1679.6009\n",
      "Test loss: 1424.4205\n",
      "Epoch: 15 Average loss: 1585.5272\n",
      "Epoch: 16 Average loss: 1418.1087\n",
      "Test loss: 1199.7971\n",
      "Epoch: 17 Average loss: 1354.7388\n",
      "Epoch: 18 Average loss: 1259.8386\n",
      "Test loss: 1071.8376\n",
      "Epoch: 19 Average loss: 1175.8477\n",
      "Epoch: 20 Average loss: 1135.9929\n",
      "Test loss: 1186.8224\n",
      "Epoch: 21 Average loss: 981.0063\n",
      "Epoch: 22 Average loss: 956.0742\n",
      "Test loss: 885.9159\n",
      "Epoch: 23 Average loss: 872.7456\n",
      "Epoch: 24 Average loss: 896.9472\n",
      "Test loss: 782.2840\n",
      "Epoch: 25 Average loss: 888.7563\n",
      "Epoch: 26 Average loss: 961.4041\n",
      "Test loss: 789.5724\n",
      "Epoch: 27 Average loss: 790.9772\n",
      "Epoch: 28 Average loss: 861.8239\n",
      "Test loss: 622.6623\n",
      "Epoch: 29 Average loss: 766.4607\n",
      "Epoch: 30 Average loss: 728.6174\n",
      "Test loss: 745.8024\n",
      "Epoch: 31 Average loss: 675.6726\n",
      "Epoch: 32 Average loss: 684.0738\n",
      "Test loss: 586.1617\n",
      "Epoch: 33 Average loss: 727.0262\n",
      "Epoch: 34 Average loss: 686.1511\n",
      "Test loss: 694.5092\n",
      "Epoch: 35 Average loss: 733.4989\n",
      "Epoch: 36 Average loss: 597.7103\n",
      "Test loss: 588.7814\n",
      "Epoch: 37 Average loss: 568.6015\n",
      "Epoch: 38 Average loss: 546.8592\n",
      "Test loss: 455.2115\n",
      "Epoch: 39 Average loss: 663.9703\n",
      "Epoch: 40 Average loss: 656.8889\n",
      "Test loss: 956.2175\n",
      "Epoch: 41 Average loss: 589.0551\n",
      "Epoch: 42 Average loss: 604.2320\n",
      "Test loss: 567.6576\n",
      "Epoch: 43 Average loss: 522.1130\n",
      "Epoch: 44 Average loss: 510.1980\n",
      "Test loss: 424.9341\n",
      "Epoch: 45 Average loss: 437.4188\n",
      "Epoch: 46 Average loss: 494.1435\n",
      "Test loss: 389.2998\n",
      "Epoch: 47 Average loss: 474.1093\n",
      "Epoch: 48 Average loss: 441.3930\n",
      "Test loss: 343.9387\n",
      "Epoch: 49 Average loss: 450.7605\n",
      "Epoch: 50 Average loss: 419.5357\n",
      "Test loss: 336.1440\n",
      "Epoch: 51 Average loss: 433.1350\n",
      "Epoch: 52 Average loss: 527.5640\n",
      "Test loss: 429.5923\n",
      "Epoch: 53 Average loss: 539.8858\n",
      "Epoch: 54 Average loss: 501.1941\n",
      "Test loss: 621.6035\n",
      "Epoch: 55 Average loss: 553.6872\n",
      "Epoch: 56 Average loss: 417.4223\n",
      "Test loss: 952.5170\n",
      "Epoch: 57 Average loss: 400.8229\n",
      "Epoch: 58 Average loss: 475.5578\n",
      "Test loss: 427.8766\n",
      "Epoch: 59 Average loss: 425.9292\n",
      "Epoch: 60 Average loss: 428.0415\n",
      "Test loss: 453.2730\n",
      "Epoch: 61 Average loss: 382.6052\n",
      "Epoch: 62 Average loss: 375.2112\n",
      "Test loss: 459.3481\n",
      "Epoch: 63 Average loss: 322.3382\n",
      "Epoch: 64 Average loss: 307.0108\n",
      "Test loss: 255.2029\n",
      "Epoch: 65 Average loss: 343.0066\n",
      "Epoch: 66 Average loss: 362.9280\n",
      "Test loss: 307.9640\n",
      "Epoch: 67 Average loss: 345.3457\n",
      "Epoch: 68 Average loss: 317.6708\n",
      "Test loss: 288.0834\n",
      "Epoch: 69 Average loss: 336.1047\n",
      "Epoch: 70 Average loss: 275.7656\n",
      "Test loss: 250.5570\n",
      "Epoch: 71 Average loss: 287.3030\n",
      "Epoch: 72 Average loss: 272.1416\n",
      "Test loss: 235.8314\n",
      "Epoch: 73 Average loss: 274.6991\n",
      "Epoch: 74 Average loss: 270.3967\n",
      "Test loss: 236.4531\n",
      "Epoch: 75 Average loss: 269.2921\n",
      "Epoch: 76 Average loss: 274.7572\n",
      "Test loss: 268.5616\n",
      "Epoch: 77 Average loss: 284.4996\n",
      "Epoch: 78 Average loss: 304.2866\n",
      "Test loss: 235.2661\n",
      "Epoch: 79 Average loss: 268.2044\n",
      "Epoch: 80 Average loss: 325.1400\n",
      "Test loss: 385.8405\n",
      "Epoch: 81 Average loss: 308.5711\n",
      "Epoch: 82 Average loss: 257.5220\n",
      "Test loss: 229.6457\n",
      "Epoch: 83 Average loss: 249.6000\n",
      "Epoch: 84 Average loss: 293.8405\n",
      "Test loss: 238.7425\n",
      "Epoch: 85 Average loss: 262.4597\n",
      "Epoch: 86 Average loss: 234.9125\n",
      "Test loss: 196.1500\n",
      "Epoch: 87 Average loss: 251.3665\n",
      "Epoch: 88 Average loss: 262.3437\n",
      "Test loss: 242.5445\n",
      "Epoch: 89 Average loss: 258.0033\n",
      "Epoch: 90 Average loss: 242.9180\n",
      "Test loss: 234.1842\n",
      "Epoch: 91 Average loss: 241.0860\n",
      "Epoch: 92 Average loss: 233.2825\n",
      "Test loss: 337.1613\n",
      "Epoch: 93 Average loss: 256.5169\n",
      "Epoch: 94 Average loss: 278.8706\n",
      "Test loss: 298.5299\n",
      "Epoch: 95 Average loss: 267.3547\n",
      "Epoch: 96 Average loss: 271.4098\n",
      "Test loss: 221.3958\n",
      "Epoch: 97 Average loss: 282.8752\n",
      "Epoch: 98 Average loss: 248.7166\n",
      "Test loss: 268.0245\n",
      "Epoch: 99 Average loss: 256.8730\n",
      "Epoch: 100 Average loss: 256.4192\n",
      "Test loss: 207.4220\n"
     ]
    }
   ],
   "source": [
    "from ava.models.vae import VAE\n",
    "save_dir = 'C:\\\\Users\\\\door1\\\\Documents\\\\Work\\\\Frankfurt\\\\autoencoded-vocal-analysis\\\\model'\n",
    "model = VAE(save_dir=save_dir)\n",
    "\n",
    "# Train.\n",
    "model.train_loop(loaders, epochs=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making field: latent_means\n",
      "Making field: latent_means\n",
      "\tDone with: latent_means\n"
     ]
    }
   ],
   "source": [
    "from ava.data.data_container import DataContainer\n",
    "import os\n",
    "model_filename = os.path.join(save_dir, 'checkpoint_100.tar')\n",
    "projection_dirs =  [\"C:\\\\Users\\\\door1\\\\Documents\\\\Work\\\\Frankfurt\\\\autoencoded-vocal-analysis\\\\data\\\\projs\"]\n",
    "dc = DataContainer(audio_dirs=audio_dirs, spec_dirs=spec_dirs,  projection_dirs=projection_dirs,    model_filename=model_filename)\n",
    "latent_means = dc.request('latent_means')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making field: latent_mean_umap\n",
      "Reading field: latent_means\n",
      "\tDone with: latent_means\n",
      "Running UMAP... (n=900)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\door1\\anaconda3\\envs\\vae\\Lib\\site-packages\\umap\\umap_.py:1945: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tDone.\n",
      "Making field: latent_mean_umap\n",
      "\tDone with: latent_mean_umap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\door1\\Documents\\Work\\Frankfurt\\autoencoded-vocal-analysis\\ava\\plotting\\latent_projection.py:212: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
      "  im = ax.scatter(X, Y, c=color, alpha=alpha, s=s, cmap=colormap)\n"
     ]
    }
   ],
   "source": [
    "from ava.plotting.latent_projection import latent_projection_plot_DC\n",
    "latent_projection_plot_DC(dc, embedding_type='latent_mean_umap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
